Telecommunication systems demand constant innovation, and the requirements grow exponentially with each generation of the wireless technology. One of the ways the 5th Generation (5G) wireless systems respond to these growing requirements is using massive Multi-Input Multi-Output (m-MIMO) antennas \cite{Dahlman2018}. These systems have the potential to improve the channel bandwidth, coverage, and capacity through beamforming and spatial multiplexing. To take advantage of these perks, the systems require accurate Channel State Information (CSI), which are metrics that describe how the channel will affect the signal. However, increasing the number of antennas increases the complexity and overhead when measuring the CSI, known as channel sounding \cite{Mawatwal2020}. These computations are built using a matrix of complex numbers, called the channel matrix, in which each element includes a gain and angle which, when multiplied by the transmitted signal will estimate the received signal (without any noise). The channel matrix grows exponentially with the addition of new antennas, further complicating channel reconstruction algorithms that try to rebuild the channel matrix based on CSI and/or received pilots. Classical algorithms will fail to meet the real-time constraints in these massive antenna systems \cite{Li2020}.

Machine Learning (ML) is an increasingly popular approach for improving the accuracy, feedback overhead, and runtime of channel reconstruction \cite{Ye2018}. 5G wireless communication systems are complex, making it difficult to implement and maintain new algorithms. Additionally, classical algorithms such as those described in \cite{Han2019} and \cite{Liu2016} have very high computational complexity. Alternatively, data-driven models eliminate the analytical complexity in the conventional methods above. ML can easily identify trends in multi-dimensional data in ways that humans cannot. Although, training a model is computationally taxing and requires a lot of data, once the model has been trained, it is very fast to execute \cite{Li2020}. ML solutions are often faster to execute and with equivalent performance, if not better, than existing solutions. 

Through this term project the authors explore channel reconstruction with simulated data. The data was generated using a simulator built with the MATLAB 5G Toolbox \cite{matlab2020}. State of the art channel reconstruction techniques were recreated using MATLAB and Python. These methods include using Least Absolute Shrinkage and Selection Operator (LASSO) \cite{Han2019} and image processing technique You Only Look Once (YOLO) \cite{Li2020}. Furthermore, the authors experimented with applying Deep Neural Network to the channel directly to predict the path gains, angles and delays. The remainder of this paper is organized as follows. First, the background section defines the wireless concepts required for this work, the existing channel reconstruction algorithms, and machine learning algorithms used. Section 3 documents the simulator and the 5G Toolbox functions used. Section 4 will explain the image generation process, along with the available configuration for the channel models Section 5-7 describes the implementation, results and challenges faced with the YOLO, LASSO, and DNN methods, respectively. Finally, Section 8 concludes the paper.  